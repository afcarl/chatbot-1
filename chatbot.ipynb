{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, getcwd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from ipywidgets import widgets  # ipywidgets should be version 7 or higher\n",
    "from IPython.display import display\n",
    "from sympy import Symbol\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from dataset import *\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "from rnn import RNN\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = path.join(ROOT_DIR, 'data')\n",
    "MODEL_DIR = path.join(ROOT_DIR, 'model')\n",
    "EMBED_DIR = path.join(MODEL_DIR, '.vector_cache')  # pre-trained embeddings\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Not using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Unpack Necessary Files, if needed\n",
    "The data we use are twitter chatlogs from https://github.com/Marsan-Ma/chat_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en.txt.gz',\n",
    "         ['twitter_en.txt.gz', 'twitter_en.txt'], DATA_DIR)\n",
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en_big.txt.gz.partaa',\n",
    "         ['twitter_en_big.txt.gz.partaa', 'twitter_en_big.txt.gz', 'twitter_en_big.txt'],\n",
    "         DATA_DIR)\n",
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en_big.txt.gz.partab',\n",
    "         ['twitter_en_big.txt.gz.partab', 'twitter_en_big.txt.gz', 'twitter_en_big.txt'],\n",
    "         DATA_DIR)\n",
    "\n",
    "# concatenate twitter_en_big.txt.gz.partaa and .partab if needed\n",
    "concatenate_two_gz(path.join(DATA_DIR, 'twitter_en_big.txt.gz'), '.partaa', '.partab')\n",
    "!chmod +w data  # make sure we have write permission in data directory\n",
    "# unzip gz files, as needed\n",
    "unzip_gz('twitter_en.txt.gz', DATA_DIR)\n",
    "unzip_gz('twitter_en_big.txt.gz', DATA_DIR)\n",
    "# create a short sample.txt file with only a few lines\n",
    "create_sample('twitter_en.txt', 'sample.txt', DATA_DIR, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and build dataset\n",
    "Create Dataset and Split to Train, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the following three lines to select data file\n",
    "FILE_NAME = 'sample.txt'  # short text file for dev (1000 lines)\n",
    "#FILE_NAME = 'twitter_en.txt'  # medium length text file (over 50MB)\n",
    "#FILE_NAME = 'twitter_en_big.txt'  # full text file (over 300MB)\n",
    "\n",
    "load_data = False  # choose whether to load data\n",
    "save_data = False  # choose whether to save data\n",
    "\n",
    "FILE_PATH = path.join(DATA_DIR, FILE_NAME)\n",
    "CHAT_DATA_NAME = 'chat_data_' + FILE_NAME[:-4] + '.p'\n",
    "CHAT_DATA_PATH = path.join(DATA_DIR, CHAT_DATA_NAME)\n",
    "EMBED_DIM = 200  # dimension of embedding vectors\n",
    "\n",
    "if load_data:\n",
    "    with open(CHAT_DATA_PATH, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "else:\n",
    "    #glove = vocab.GloVe('twitter.27B', dim=EMBED_DIM, cache=EMBED_DIR)\n",
    "    dataset = ChatDataset(data_path = FILE_PATH,  # path to data tile\n",
    "                          max_length = 25,  # maximum length of sentence\n",
    "                          max_vocab_size = 8000,  # maximum size of vocabulary\n",
    "                          min_freq = 5,  # minimum frequency to add word to vocabulary\n",
    "                          sos_token = '<sos>',  # token that tells decoder to start response\n",
    "                          eos_token = '<eos>',  # end of sentence token\n",
    "                          pad_token = '<pad>',  # padding to keep sentence lengths equal\n",
    "                          unk_token = '<unk>',  # unknown word (word not in vocabulary)\n",
    "                          special_tokens = [],  # any other tokens to add to vocabulary\n",
    "                          embed_dim = EMBED_DIM)  # dimension of embedding vectors\n",
    "                          #pre_trained = glove)  # pre_trained word embeddings\n",
    "\n",
    "    if save_data:\n",
    "        with open(CHAT_DATA_PATH, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "print(\"Number of words in vocabulary: %d\" % dataset.nwords)\n",
    "print(\"Number of sentences in data: %d\\n\" % len(dataset))\n",
    "\n",
    "# Split to 60% training, 20% validation, and 20% test set\n",
    "train_sampler, valid_sampler, test_sampler = split_data(dataset, 0.6, 0.2, 0.2)\n",
    "\n",
    "print(\"Training set size: %d\" % len(train_sampler))\n",
    "print(\"Validation set size: %d\" % len(valid_sampler))\n",
    "print(\"Test set size: %d\" % len(test_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(RNN):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, bidirect=True):\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, True)  # encoder uses bidrectional RNN\n",
    "        self.init_weights()  # initialize weights when initializing rnn\n",
    "        #self.embedding.weight.data = dataset.vocab.vectors\n",
    "\n",
    "    def forward(self, input, hidden, lengths=None, max_len=None):\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input)\n",
    "        if lengths is not None:  # run all time step at once (evalution mode)\n",
    "            output = pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "            # unpack packed sequence for use in decoder with attention\n",
    "            output, out_lens = pad_packed_sequence(output, batch_first=True)\n",
    "            # refill padding\n",
    "            n_pad = max_len - max(out_lens)\n",
    "            if n_pad > 0:\n",
    "                padding = Variable(torch.zeros(batch_size, n_pad, self.hidden_size))\n",
    "                padding = padding.cuda() if self.is_cuda() else padding\n",
    "                output = torch.cat((output, padding), 1)\n",
    "        else:  # running one time step at a time (training mode)\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "            output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(RNN):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, bidirect=False):\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, False)  # decoder is not bidirectional\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_weights()\n",
    "        #self.embedding.weight.data = dataset.vocab.vectors\n",
    "\n",
    "    def init_weights(self):\n",
    "        super().init_weights()\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        output = self.relu(embedded)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.linear(output[:, 0, :])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttDecoder(RNN):\n",
    "    \"\"\"\n",
    "    Decoder with attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, bidirect=False):\n",
    "        if nlayers != 2:\n",
    "            raise ValueError(\"AttentionDecoderRNN supports 2 layers.\")\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, False)\n",
    "        # learned linear mapping: previous hidden > context vector\n",
    "        self.prevhid2context = nn.Linear(hidden_size, hidden_size)\n",
    "        # learned linear mapping: encoder outputs > context vector\n",
    "        self.encoder2context = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.v = Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        super().init_weights()\n",
    "        init_range = 0.1\n",
    "        self.prevhid2context.weight.data.uniform_(-init_range, init_range)\n",
    "        self.encoder2context.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out.weight.data.uniform_(-init_range, init_range)\n",
    "        self.v.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, prev_hidden, encoder_out):\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        # set prev_hidden to highest previous hidden layer\n",
    "        if isinstance(prev_hidden, tuple):  # encoder was LSTM\n",
    "            prev_hidden[0].data = prev_hidden[0][-1].data\n",
    "            align1 = self.prevhid2context(prev_hidden[0]).unsqueeze(1)\n",
    "        else:  # encoder was GRU\n",
    "            prev_hidden = prev_hidden[-1]\n",
    "            align1 = self.prevhid2context(prev_hidden).unsqueeze(1)\n",
    "\n",
    "        align2 = self.encoder2context(encoder_out)\n",
    "        score_fn = torch.matmul(self.tanh(align1 + align2), self.v)\n",
    "        # scores of how much each encoder ouput should be seen\n",
    "        scores = self.softmax(score_fn)\n",
    "        # weighted sum of scores and encoder output over sentence indexes\n",
    "        context_vector = (scores * encoder_out).sum(1)\n",
    "\n",
    "        if isinstance(prev_hidden, tuple):\n",
    "            context = torch.stack((context_vector, prev_hidden[0]))\n",
    "            prev_hidden[0].data = context.data\n",
    "        else:\n",
    "            context = torch.stack((context_vector, prev_hidden))\n",
    "            prev_hidden = context\n",
    "        output, hidden = self.rnn(embedded, prev_hidden)\n",
    "        output = self.softmax(self.out(output[:, 0, :]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sequence to Sequence Model\n",
    "First, we initialize the necessary components of the model, including the encoder, decoder, optimizer and learning rate scheduler of optimizer.\n",
    "\n",
    "To regularize the model, we apply dropout. We apply the same dropout mask, for each time step, and the same dropout mask at the word embeddings dropout, following Gal and Ghahramani, A Theoretically Grounded Application of Dropout in Recurrent Neural Networks, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NHIDDEN = 256  # hidden layer size\n",
    "NLAYERS = 2  # number of recurrent layers\n",
    "EMBED_DIM = dataset.embed_dim  # dimension of embedding vectors\n",
    "LEARNING_RATE = 0.001  # initial learning rate\n",
    "DROP_T = 0.3  # probability of applying dropout across time steps\n",
    "DROP_E = 0.01  # probability of dropping a word\n",
    "\n",
    "encoder = EncoderRNN(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU', dataset.pad_idx)\n",
    "decoder = AttDecoder(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU', dataset.pad_idx)\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "\n",
    "w = torch.ones(dataset.nwords)\n",
    "# Losses from unk token or eos token contribute less relatively to other tokens\n",
    "w[dataset.unk_idx], w[dataset.eos_idx] = 0.01, 0.1\n",
    "# Do not calculate loss when target is padding, encouraging model to say more\n",
    "loss = nn.NLLLoss(ignore_index=dataset.pad_idx, size_average=False, weight=w)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    loss = loss.cuda()\n",
    "\n",
    "# need to create optimizer after .cuda() call on models\n",
    "optimizer = optim.SGD(params, lr=LEARNING_RATE)\n",
    "# halve lr if validation loss is not reduced after patience*plot_every epochs\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "\n",
    "best_val_loss = float('inf')  # best validation loss so far\n",
    "SAVE_MODEL = False  # choose whether to save best performing model on disk\n",
    "ENCODER_SAVE_PATH = path.join(MODEL_DIR, 'encoder.pth')\n",
    "DECODER_SAVE_PATH = path.join(MODEL_DIR, 'decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help the model to learn, we employ teacher forcing, whereby we feed the target labels as input at each time step to the decoder. Initially, we feed the target labels frequently. However, as the model continues to train, we decrease our use of teacher forcing. The teacher forcing probabilities will be given by the inverse sigmoid function and teacher forcing decisions are made at each time step following, Bengio et al. Scheduled Sampling for Sequence Prediction with\n",
    "Recurrent Neural Networks, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEPOCH = 3000  # number of epochs\n",
    "# k_val is the rate of convergence of teacher forcing probability to 0.\n",
    "# initially, choose high k_val, but on subsequent runs of training cell, choose low k_val\n",
    "k_val = 300\n",
    "\n",
    "# Set teacher forcing probabilities\n",
    "x_list = np.linspace(1, NEPOCH + 1, num=NEPOCH, dtype=np.int)\n",
    "tf_probs = np.zeros(NEPOCH)\n",
    "draws = torch.rand(NEPOCH, dataset.max_len)\n",
    "choices = torch.ByteTensor(NEPOCH, dataset.max_len)\n",
    "\n",
    "with ProcessPoolExecutor() as executer:\n",
    "    for i in range(NEPOCH):\n",
    "        tf_probs[i] = inv_sigm_eval(x_list[i], k_val)\n",
    "        choices[i] = torch.le(draws[i], tf_probs[i])\n",
    "\n",
    "if USE_CUDA:\n",
    "    choices = choices.cuda()\n",
    "\n",
    "plot(x_list, tf_probs, 'epoch', 'teacher forcing probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train sequence to sequence model.\n",
    "Interrupt Kernal at any time to stop training before completion.\n",
    "Run cell again to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH = 20  # batch size for training\n",
    "# volatile flag allows greater validation set batch size\n",
    "if (len(valid_sampler) // (TRAIN_BATCH * 20)) >= 1:\n",
    "    VALID_BATCH = TRAIN_BATCH * 20\n",
    "else:\n",
    "    VALID_BATCH = TRAIN_BATCH\n",
    "\n",
    "valid_batches = DataLoader(dataset, batch_size=VALID_BATCH,\n",
    "                           sampler=valid_sampler, num_workers=0,\n",
    "                           collate_fn=collate_fn, pin_memory=True,\n",
    "                           drop_last=True)\n",
    "\n",
    "# Setting up information to print and plot\n",
    "plot_every = 200  # plot one data point per plot_every training losses\n",
    "print_every = 500  # frequency of printing loss information\n",
    "train_loss_plot, train_loss_print = 0, 0\n",
    "train_losses, valid_losses, epoch_list = [], [], []\n",
    "\n",
    "try:\n",
    "    encoder.load_state_dict(best_encoder)\n",
    "    decoder.load_state_dict(best_decoder)\n",
    "    print(\"Loaded up weights from previous training.\")\n",
    "except NameError:\n",
    "    print(\"This is the first run of this cell, starting training procedure.\")\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm_notebook(range(1, NEPOCH + 1), unit=' epochs'):\n",
    "        train_loss = train(encoder, decoder, TRAIN_BATCH, train_sampler,\n",
    "                           optimizer, params, dataset, choices[epoch-1],\n",
    "                           loss, DROP_T, DROP_E)\n",
    "        train_loss_plot += train_loss\n",
    "        train_loss_print += train_loss\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            # Calculate validation loss\n",
    "            val_loss = evaluate(encoder, decoder, VALID_BATCH,\n",
    "                                valid_batches, dataset, loss)\n",
    "            scheduler.step(val_loss)  # update learning rate scheduler\n",
    "            # Update losses to plot\n",
    "            train_avg = train_loss_plot / plot_every\n",
    "            train_losses.append(train_avg)\n",
    "            valid_losses.append(val_loss)\n",
    "            epoch_list.append(epoch)\n",
    "            train_loss_plot = 0\n",
    "\n",
    "            # Copy model if validation loss is the best so far\n",
    "            if best_val_loss > val_loss and epoch > (NEPOCH // 2):\n",
    "                best_bal_loss = val_loss\n",
    "                best_encoder = encoder.state_dict()\n",
    "                best_decoder = decoder.state_dict()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            train_avg = train_loss_print / print_every\n",
    "            print('Epoch: %d  Training loss: %.4f, Validation loss %.4f'\n",
    "                  % (epoch, train_avg, valid_losses[-1]))\n",
    "            train_loss_print = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped. Run cell again to continue training.\")\n",
    "\n",
    "if len(epoch_list) > 5:\n",
    "    plot(epoch_list, train_losses, 'epoch', 'training loss')\n",
    "    plot(epoch_list, valid_losses, 'epoch', 'validation loss')\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    torch.save(best_encoder, ENCODER_SAVE_PATH)\n",
    "    torch.save(best_decoder, DECODER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU', dataset.pad_idx)\n",
    "decoder = AttDecoder(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU', dataset.pad_idx)\n",
    "# Load best model so far\n",
    "try:\n",
    "    encoder.load_state_dict(best_encoder)\n",
    "    decoder.load_state_dict(best_decoder)\n",
    "except NameError:  # training cell was not run previously\n",
    "    try:\n",
    "        encoder.load_state_dict(torch.load(ENCODER_SAVE_PATH))\n",
    "        decoder.load_state_dict(torch.load(DECODER_SAVE_PATH))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No model found. Recommend running the training segment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = widgets.Text()\n",
    "display(text)\n",
    "\n",
    "\n",
    "def chat(sender):\n",
    "    try:\n",
    "        output = respond(encoder, decoder, text.value, dataset)\n",
    "        if len(output) == 0:\n",
    "            output = \"Sorry, I didn't get you. Let's about something else.\"\n",
    "    except UserInputTooLongError:\n",
    "        output = \"Calm down, please talk a bit slower.\"\n",
    "    print(\"You: %s\" % text.value)\n",
    "    print(\"Bot: %s\" % output)\n",
    "\n",
    "text.on_submit(chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
