{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, getcwd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from ipywidgets import widgets  # ipywidgets should be version 7 or higher\n",
    "from IPython.display import display\n",
    "from sympy import Symbol\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn.utils import weight_norm\n",
    "from dataset import *\n",
    "from data_utils import *\n",
    "from model_utils import *\n",
    "from rnn import RNN\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = path.join(ROOT_DIR, 'data')\n",
    "MODEL_DIR = path.join(ROOT_DIR, 'model')\n",
    "EMBED_DIR = path.join(MODEL_DIR, '.vector_cache')  # pre-trained embeddings\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Not using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Unpack Necessary Files, if needed\n",
    "The data we use are twitter chatlogs from https://github.com/Marsan-Ma/chat_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en.txt.gz',\n",
    "         ['twitter_en.txt.gz', 'twitter_en.txt'], DATA_DIR)\n",
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en_big.txt.gz.partaa',\n",
    "         ['twitter_en_big.txt.gz.partaa', 'twitter_en_big.txt.gz', 'twitter_en_big.txt'],\n",
    "         DATA_DIR)\n",
    "download('https://github.com/Marsan-Ma/chat_corpus/raw/master/twitter_en_big.txt.gz.partab',\n",
    "         ['twitter_en_big.txt.gz.partab', 'twitter_en_big.txt.gz', 'twitter_en_big.txt'],\n",
    "         DATA_DIR)\n",
    "\n",
    "# concatenate twitter_en_big.txt.gz.partaa and .partab if needed\n",
    "concatenate_two_gz(path.join(DATA_DIR, 'twitter_en_big.txt.gz'), '.partaa', '.partab')\n",
    "!chmod +w data  # make sure we have write permission in data directory\n",
    "# unzip gz files, as needed\n",
    "unzip_gz('twitter_en.txt.gz', DATA_DIR)\n",
    "unzip_gz('twitter_en_big.txt.gz', DATA_DIR)\n",
    "# create a short sample.txt file with only a few lines\n",
    "create_sample('twitter_en.txt', 'sample.txt', DATA_DIR, 20000)\n",
    "!mkdir model  # create directory for saving models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and build dataset\n",
    "Create Dataset and Split to Train, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the following three lines to select data file\n",
    "FILE_NAME = 'sample.txt'  # short text file for dev\n",
    "#FILE_NAME = 'twitter_en.txt'  # medium length text file (754530 lines)\n",
    "#FILE_NAME = 'twitter_en_big.txt'  # full text file (5202488 lines)\n",
    "\n",
    "load_data = False  # choose whether to load data\n",
    "save_data = False  # choose whether to save data\n",
    "\n",
    "FILE_PATH = path.join(DATA_DIR, FILE_NAME)\n",
    "CHAT_DATA_NAME = 'chat_data_' + FILE_NAME[:-4] + '.p'\n",
    "CHAT_DATA_PATH = path.join(DATA_DIR, CHAT_DATA_NAME)\n",
    "EMBED_DIM = 200  # dimension of embedding vectors\n",
    "\n",
    "if load_data:\n",
    "    with open(CHAT_DATA_PATH, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "else:\n",
    "    #glove = vocab.GloVe('twitter.27B', dim=EMBED_DIM, cache=EMBED_DIR)\n",
    "    dataset = ChatDataset(data_path = FILE_PATH,  # path to data tile\n",
    "                          max_length = 12,  # maximum length of sentence\n",
    "                          max_vocab_size = 8000,  # maximum size of vocabulary\n",
    "                          min_freq = 6,  # minimum frequency to add word to vocabulary\n",
    "                          eos_token = '<eos>',  # end of sentence token\n",
    "                          pad_token = '<pad>',  # padding to keep sentence lengths equal\n",
    "                          unk_token = '<unk>',  # unknown word (word not in vocabulary)\n",
    "                          special_tokens = [],  # any other tokens to add to vocabulary\n",
    "                          embed_dim = EMBED_DIM,  # dimension of embedding vectors\n",
    "                          threshold = 3)  # count of unk required to remove sentence\n",
    "                          #pre_trained = glove)  # pre_trained word embeddings\n",
    "\n",
    "    if save_data:\n",
    "        with open(CHAT_DATA_PATH, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "print(\"Number of words in vocabulary: %d\" % dataset.nwords)\n",
    "print(\"Number of sentences in data: %d\" % len(dataset))\n",
    "print(\"Number of unknown words in data: %d\" % dataset.unk_count)\n",
    "print(\"Total number of words in data: %d\\n\" % dataset.total_tokens)\n",
    "\n",
    "# Split to training, validation, and test set\n",
    "train_sampler, valid_sampler, test_sampler = split_data(dataset, 0.6, 0.2, 0.2)\n",
    "\n",
    "print(\"Training set size: %d\" % len(train_sampler))\n",
    "print(\"Validation set size: %d\" % len(valid_sampler))\n",
    "print(\"Test set size: %d\" % len(test_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(RNN):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, use_cuda, dropout, bidirect=True):\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, use_cuda, dropout, True)  # bidrectional\n",
    "        self.init_weights()  # initialize weights when initializing rnn\n",
    "\n",
    "    def forward(self, input, hidden, lengths, max_len):\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input)\n",
    "        output = pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        # unpack packed sequence for use in decoder with attention\n",
    "        output, out_lens = pad_packed_sequence(output, batch_first=True)\n",
    "        # refill padding\n",
    "        n_pad = max_len - max(out_lens)\n",
    "        if n_pad > 0:\n",
    "            padding = Variable(torch.zeros(batch_size, n_pad, self.hidden_size))\n",
    "            padding = padding.cuda() if self.is_cuda() else padding\n",
    "            output = torch.cat((output, padding), 1)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(RNN):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, use_cuda, dropout, bidirect=False):\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, use_cuda, dropout, False)  # unidirectional\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        super().init_weights()\n",
    "        self.linear.weight.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def forward(self, input, hidden, lengths=None):\n",
    "        batch_size = input.size()[0]\n",
    "        embedded = self.embedding(input).unsqueeze(1)            \n",
    "        output = self.relu(embedded)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.linear(output[:, 0, :])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Decoder model with Attention, where we wish to learn how much to peek into the encoder's topmost hidden state to gain some additional information. In order to learn where to look and how much to look, we the scoring function $\\mathbf{h}_t^T \\mathbf{W} \\mathbf{\\bar{h}}_s$ from Luong et al, Effective Approaches to Attention-based Neural Machine Translation, 2015. The first $\\mathbf{h}$ is the top hidden layer from the previous time step and the second $\\mathbf{\\bar{h}}$ is a top hidden layer from the encoder.\n",
    "\n",
    "With the attention mechanism in place, whenever the decoder outputs the unknown word token, we can replace the output with the input word given at the time index with the highest attention score, following Luong et al, Addressing the Rare Word Problem in Neural Machine Translation, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttDecoder(RNN):\n",
    "    \"\"\"\n",
    "    Decoder with attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, embed_dim,\n",
    "                 rnn_type, pad_idx, max_len, use_cuda, dropout, bidirect=False):\n",
    "        super().__init__(input_size, hidden_size, nlayers, embed_dim,\n",
    "                         rnn_type, pad_idx, use_cuda, dropout, False)\n",
    "        self.seq_len = max_len\n",
    "        # function to calculate scores of which time step to attend\n",
    "        self.score_fn = nn.Bilinear(hidden_size, hidden_size, 1, False)\n",
    "        # calculates weights of how much each time step to attend\n",
    "        self.score_softmax = nn.Softmax()\n",
    "\n",
    "        # Intermediate linear mapping between output embedding and rnn\n",
    "        # in order to tie input and output embedding\n",
    "        self.linear = nn.Linear(hidden_size, embed_dim, bias=False)\n",
    "\n",
    "        self.output_embed = nn.Linear(embed_dim, input_size, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        super().init_weights()\n",
    "        init_range = 0.05\n",
    "        self.score_fn.weight.data.uniform_(-init_range, init_range)\n",
    "        self.linear.weight.data.uniform_(-init_range, init_range)\n",
    "        self.output_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        weight_norm(self.score_fn)\n",
    "\n",
    "    def forward(self, input, prev_hidden, encoder_hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Variable): input words to go into rnn\n",
    "            prev_hidden (Variable): hidden layer from previous time step\n",
    "            encoder_hidden (Variable) : top level hidden layers from encoder\n",
    "\n",
    "        Dimensions:\n",
    "            input: batch size * 1 (one word at a time)\n",
    "            prev_hidden: number of layers * batch size * hidden size\n",
    "            encoder_hidden: batch size * max sentence length * hidden size\n",
    "        \"\"\"\n",
    "        batch_size = input.size()[0]\n",
    "        # input is one word, so unsqueeze second dimension (sequence length)\n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        # get highest previous hidden layer\n",
    "        # and get lower hidden layer, which will be directly fed this time step\n",
    "        if isinstance(prev_hidden, tuple):  # encoder was LSTM\n",
    "            top_prev_hidden = prev_hidden[0][-1]\n",
    "            if self.nlayers > 1:\n",
    "                lower_prev_hidden = prev_hidden[0][1:]\n",
    "        else:  # encoder was GRU\n",
    "            top_prev_hidden = prev_hidden[-1]\n",
    "            if self.nlayers > 1:\n",
    "                lower_prev_hidden = prev_hidden[1:]\n",
    "\n",
    "        # container for scores\n",
    "        scores = Variable(torch.zeros(batch_size, self.seq_len), requires_grad=False)\n",
    "        scores = scores.cuda() if self.is_cuda() else scores\n",
    "        # calculate pairwise scores of encoder hidden at time i and top_prev_hidden\n",
    "        for i in range(self.seq_len):\n",
    "            scores[:, i] = self.score_fn(top_prev_hidden, encoder_hidden[:, i])\n",
    "        # normalize scores to get probabilites, which will be used to weigh\n",
    "        # relevence of encoder hidden layer at time i\n",
    "        # Dimension is: batch_size * 1 * seq_len\n",
    "        normalized_scores = self.score_softmax(scores).unsqueeze(1)\n",
    "        # weighted sum of scores and encoder output over sentence indexes\n",
    "        # Dimension is: batch_size * 1 * hidden_size\n",
    "        context_vector = torch.bmm(normalized_scores, encoder_hidden)\n",
    "        # Rearrange such that number of layers (1) comes first\n",
    "        context_vector = context_vector.permute(1, 0, 2)\n",
    "        # Finally, we set current highest hidden state to the context vector\n",
    "        if self.nlayers > 1:\n",
    "            curr_hidden = torch.cat((context_vector, lower_prev_hidden), 0)\n",
    "        else:\n",
    "            curr_hidden = context_vector\n",
    "\n",
    "        if isinstance(prev_hidden, tuple):\n",
    "            prev_hidden[0].data = curr_hidden.data\n",
    "        else:\n",
    "            prev_hidden.data = curr_hidden.data\n",
    "\n",
    "        output, hidden = self.rnn(embedded, prev_hidden)\n",
    "        output = self.softmax(self.output_embed(self.linear(output[:, 0, :])))\n",
    "\n",
    "        # return index with highest attention score\n",
    "        _, max_atten_idx = torch.max(scores[0], 0)\n",
    "        return output, hidden, max_atten_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sequence to Sequence Model\n",
    "First, we initialize the necessary components of the model, including the encoder, decoder, optimizer and learning rate scheduler of optimizer.\n",
    "\n",
    "We employ three methods to regularize out model.\n",
    "We have dropout on connections between each layer at any one time step are drawn independently at each time step just like in feed forward nets, as in Melis et al. On the State of the Art of Evaluation in Neural Language Models, 2017.\n",
    "We also tie the weights of the encoder embedding matrix and the decoder linear output mapping matrix, following Press and World, Using the Output Embedding to Improve Language Models, 2016.\n",
    "In addition, we employ weight decay to prevent weights from becoming to big.\n",
    "\n",
    "Since we use a bidirectional encoder, we do not use variational dropout from Gal and Ghahramani 2016 (keeping the same dropout mask with each time step between hidden states), as implementing variational dropout for a bidirectional encoder will require major workarounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NHIDDEN = 514  # hidden layer size\n",
    "NLAYERS = 2  # number of recurrent layers\n",
    "EMBED_DIM = dataset.embed_dim  # dimension of embedding vectors\n",
    "LEARNING_RATE = 0.001  # initial learning rate\n",
    "DROP_L = 0.2  # probability of dropping connections between layers\n",
    "\n",
    "encoder = EncoderRNN(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU',\n",
    "                     dataset.pad_idx, USE_CUDA, DROP_L)\n",
    "decoder = AttDecoder(dataset.nwords, NHIDDEN, NLAYERS, EMBED_DIM, 'GRU',\n",
    "                     dataset.pad_idx, dataset.max_len, USE_CUDA, DROP_L)\n",
    "# Use pre-trained embedding weights, if needed\n",
    "#encoder.embedding.weight = dataset.vocab.vectors\n",
    "#decoder.embedding.weight = dataset.vocab.vectors\n",
    "# Tie input embedding and output embedding\n",
    "encoder.embedding.weight = decoder.output_embed.weight\n",
    "\n",
    "# Do not calculate loss when target is padding, encouraging model to say more\n",
    "loss = nn.NLLLoss(ignore_index=dataset.pad_idx, size_average=False)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    loss = loss.cuda()\n",
    "\n",
    "# need to create optimizer after .cuda() call on models\n",
    "e_opt = optim.SGD(encoder.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
    "d_opt = optim.SGD(decoder.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
    "# halve lr if validation loss is not reduced after patience*plot_every epochs\n",
    "e_sched = ReduceLROnPlateau(e_opt, factor=0.5, patience=5)\n",
    "d_sched = ReduceLROnPlateau(d_opt, factor=0.5, patience=5)\n",
    "\n",
    "best_val_loss = float('inf')  # best validation loss so far\n",
    "SAVE_MODEL = False  # choose whether to save best performing model on disk\n",
    "ENCODER_SAVE_PATH = path.join(MODEL_DIR, 'encoder.pth')\n",
    "DECODER_SAVE_PATH = path.join(MODEL_DIR, 'decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help the model to learn, we employ teacher forcing, whereby we feed the target labels as input at each time step to the decoder. Initially, we feed the target labels frequently. However, as the model continues to train, we decrease our use of teacher forcing. The teacher forcing probabilities will be given by the inverse sigmoid function and teacher forcing decisions are made at each time step following, Bengio et al. Scheduled Sampling for Sequence Prediction with\n",
    "Recurrent Neural Networks, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPOCH = 30000  # number of epochs\n",
    "# k_val is the rate of convergence of teacher forcing probability to 0.\n",
    "# initially, choose high k_val, but on subsequent runs of training cell, choose low k_val\n",
    "k_val = 2000\n",
    "\n",
    "# Set teacher forcing probabilities\n",
    "x_list = np.linspace(1, NEPOCH + 1, num=NEPOCH, dtype=np.int)\n",
    "tf_probs = k_val / (np.exp(x_list / k_val) + k_val)\n",
    "draws = torch.rand(NEPOCH, dataset.max_len)\n",
    "choices = torch.ByteTensor(NEPOCH, dataset.max_len)\n",
    "\n",
    "with ProcessPoolExecutor() as executer:\n",
    "    for i in range(NEPOCH):\n",
    "        choices[i] = torch.le(draws[i], tf_probs[i])\n",
    "\n",
    "plot(x_list, tf_probs, 'epoch', 'teacher forcing probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train sequence to sequence model.\n",
    "Interrupt Kernal at any time to stop training before completion.\n",
    "Run cell again to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH = 32  # batch size for training\n",
    "# volatile flag allows greater validation set batch size\n",
    "if (len(valid_sampler) // (TRAIN_BATCH * 20)) >= 1:\n",
    "    VALID_BATCH = TRAIN_BATCH * 20\n",
    "else:\n",
    "    VALID_BATCH = len(valid_sampler)\n",
    "\n",
    "train_batches = DataLoader(dataset, batch_size=TRAIN_BATCH,\n",
    "                           sampler=train_sampler, collate_fn=collate_fn,\n",
    "                           pin_memory=True,\n",
    "                           drop_last=True)\n",
    "valid_batches = DataLoader(dataset, batch_size=VALID_BATCH,\n",
    "                           sampler=valid_sampler, collate_fn=collate_fn,\n",
    "                           pin_memory=True, drop_last=True)\n",
    "ntrain_batches = len(train_batches)\n",
    "nvalid_batches = len(valid_batches)\n",
    "\n",
    "# Setting up information to print and plot\n",
    "plot_every = 50  # plot one data point per plot_every training losses\n",
    "print_every = 50  # frequency of printing loss information\n",
    "train_loss_plot, train_loss_print = 0, 0\n",
    "train_losses, valid_losses, epoch_list = [], [], []\n",
    "\n",
    "try:\n",
    "    encoder.load_state_dict(best_encoder)\n",
    "    decoder.load_state_dict(best_decoder)\n",
    "    print(\"Loaded up weights from previous training.\")\n",
    "except NameError:\n",
    "    print(\"First run of this training cell.\")\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm_notebook(range(1, NEPOCH + 1), unit=' epochs'):\n",
    "        choice = choices[epoch-1]\n",
    "        if encoder.is_cuda():\n",
    "            choice = choice.cuda()\n",
    "        train_loss = 0\n",
    "        train_loss += train(encoder, decoder, TRAIN_BATCH, train_batches,\n",
    "                            e_opt, d_opt, dataset, choice, loss)\n",
    "        train_loss /= ntrain_batches\n",
    "        del choice\n",
    "        train_loss_plot += train_loss\n",
    "        train_loss_print += train_loss\n",
    "        if epoch % plot_every == 0:\n",
    "            # Calculate validation loss\n",
    "            val_loss = evaluate(encoder, decoder, VALID_BATCH,\n",
    "                                valid_batches, dataset, loss)\n",
    "            val_loss /= nvalid_batches\n",
    "            # update learning rate scheduler\n",
    "            e_sched.step(val_loss)\n",
    "            d_sched.step(val_loss)\n",
    "            # Update losses to plot\n",
    "            train_avg = train_loss_plot / plot_every\n",
    "            train_losses.append(train_avg)\n",
    "            valid_losses.append(val_loss)\n",
    "            epoch_list.append(epoch)\n",
    "            train_loss_plot = 0\n",
    "\n",
    "            # Copy model if validation loss is the best so far\n",
    "            if best_val_loss > val_loss and epoch > (NEPOCH // 2):\n",
    "                best_val_loss = val_loss\n",
    "                best_encoder = encoder.state_dict()\n",
    "                best_decoder = decoder.state_dict()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            train_avg = train_loss_print / print_every\n",
    "            valid_loss_print = valid_losses[-1] if len(valid_losses) > 0 else 0\n",
    "            print('Epoch: %d  Avg Training loss: %.4f, Avg Validation loss %.4f'\n",
    "                  % (epoch, train_avg, valid_loss_print))\n",
    "            train_loss_print = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped. Run cell again to continue training.\")\n",
    "\n",
    "if len(epoch_list) > 5:\n",
    "    plot(epoch_list, train_losses, 'epoch', 'training loss')\n",
    "    plot(epoch_list, valid_losses, 'epoch', 'validation loss')\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    torch.save(best_encoder, ENCODER_SAVE_PATH)\n",
    "    torch.save(best_decoder, DECODER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample responses from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best performing model so far\n",
    "try:\n",
    "    encoder.load_state_dict(best_encoder)\n",
    "    decoder.load_state_dict(best_decoder)\n",
    "except NameError:  # training cell was not run previously\n",
    "    try:\n",
    "        encoder.load_state_dict(torch.load(ENCODER_SAVE_PATH))\n",
    "        decoder.load_state_dict(torch.load(DECODER_SAVE_PATH))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No model found. Recommend running the training segment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random subset of test data\n",
    "test_batches = DataLoader(dataset, batch_size=5, sampler=test_sampler,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "test_batch = next(iter(test_batches))\n",
    "test_input, _, input_lens = test_batch\n",
    "\n",
    "for i in range(test_input.size()[0]):\n",
    "    response = respond(encoder, decoder, test_input[i], dataset, [input_lens[i]])\n",
    "    # get human readable words from input embedding indexes\n",
    "    input_line = get_input(test_input[i], dataset)\n",
    "    print(\"Input: %s\" % input_line)\n",
    "    print(\"Response: %s\\n\" % response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = widgets.Text()\n",
    "display(text)\n",
    "\n",
    "\n",
    "def chat(sender):\n",
    "    try:\n",
    "        output = respond(encoder, decoder, text.value, dataset)\n",
    "        if len(output) == 0:\n",
    "            output = \"Sorry, I didn't get you. Let's about something else.\"\n",
    "    except UserInputTooLongError:\n",
    "        output = \"Calm down, please talk a bit slower.\"\n",
    "    print(\"You: %s\" % text.value)\n",
    "    print(\"Bot: %s\" % output)\n",
    "\n",
    "text.on_submit(chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
